{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FastAI Chapter 5 - summary"},{"metadata":{},"cell_type":"markdown","source":"In chapter 5 the book expands the model built in Ch4 (3s and 7s) to more than 2 categories. In this example the task is to classify 37 different dog breeds, hence for a loss function we will need an activation for each category with the following properties: <br> \n- values btw 0 and 1 <br>\n- differentiable <br>\n- each category sums up to 1 <br>\n<br>\n\nThe solution to this is the cross entropy loss function. \nThis chapter also uses regex in order to identify filenames to label the whole image dataset."},{"metadata":{},"cell_type":"markdown","source":"So let's start with label the dog breed dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\nfrom fastbook import *\nfrom fastai.vision.all import *\n\npath = untar_data(URLs.PETS)\nPath.BASE_PATH = path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the 'annotation' folder which is included in the pets dataset comprises locations rather than breed we will extract the labelling information from the file names:"},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall(r'(.+)_\\d+.jpg$', fname.name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use this as get_y labelling method in the DataBlock class using the RegesLabelling class. We are also using the block API. Reminder: DataBlock helps to then quickly create Datasets and DataLoaders."},{"metadata":{"trusted":true},"cell_type":"code","source":"pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\n\nhelp(DataBlock)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since Datablock is the foundation of everything to come we have to make sure it does what we want it to do. So we need to check and debugg. First we will take a look at one batch:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.show_batch(nrows=1, ncols=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can debug the DataBlock using the 'summary' method. In the following example we forgot to include the Resize function in the Datablock which leads to images with different size, hence an error when collating the tensor."},{"metadata":{"trusted":true},"cell_type":"code","source":"#hide_output\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As ground truth we should train a simple model and check the results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note! If we don't specifiy a loss function explicitly fastAI will try to chose an apropriate one. In case of images this is usually 'cross-entropy'.\n\n# Cross-Entropy\n\nCross-Entropy is a combination of 'soft-max' and log-likelihood. softmax is scaling the activation between 0 and 1 and ensures that the sum of the activations of all categories adds up to 1. \nThe log is going to emphasise small changes close to the optimum (e.g.: to get from 0.99 accuracy to 0.999). <br>\nThe corresponing class is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn.CrossEntropyLoss(reduction='none')(acts, targ)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model interpretation\nIn order to interpret the result of the model we can use a confusion matrix. This can get confusing is cases of a lot of categories. But we can look at the most confused categories using: "},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learn rate finder\nIn order to find the right 'lr' we use the following function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This will increases the learn rate step by step and computes the loss. Once the loss increases the learn rate is too high.  "},{"metadata":{},"cell_type":"markdown","source":"# Freezing and tranfer learning\nWhen we fine-tune a network we throw out the last or last few layers, put on new ones with randomly initialized weights, freeze the parameters of the original layers while training the new layers for a few epochs and then unfreeze all parameters for the following epochs of training. "},{"metadata":{},"cell_type":"markdown","source":"# Discriminative Learning"},{"metadata":{},"cell_type":"markdown","source":"Different learning rate for different layers in case of transfer learning. Pretrained layers will not need a learning rate as high as the new layers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}